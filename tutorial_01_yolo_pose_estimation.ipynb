{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Human Pose Estimation with YOLO & Ultralytics\n",
    "\n",
    "**Pipeline Stage:** 2D Pose Estimation from Multi-Camera Video\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial walks you through the **first stage** of our multi-camera human tracking pipeline:\n",
    "extracting 2D human pose (skeleton) keypoints from video using the **YOLOv8 pose estimation model**.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "1. **Installation** — How to install Ultralytics, OpenCV, SLEAP-IO, and all dependencies\n",
    "2. **Frame Extraction** — How to pull individual frames from video at specific timestamps\n",
    "3. **Single-Frame Inference** — Running YOLO pose estimation on a single image\n",
    "4. **Understanding the Output** — Bounding boxes, keypoints, confidence scores, and the COCO skeleton\n",
    "5. **Batch Video Processing** — Running pose estimation across an entire video\n",
    "6. **Handling Portrait/Rotated Cameras** — Rotating frames before inference and mapping keypoints back\n",
    "7. **Exporting Results** — Saving to SLEAP (.slp), JSON, and Pickle formats\n",
    "\n",
    "### Pipeline Context\n",
    "\n",
    "```\n",
    "┌─────────────────────┐     ┌──────────────────────┐     ┌─────────────────────┐\n",
    "│  Tutorial 1 (HERE)  │ ──► │  Tutorial 2           │ ──► │  Tutorial 3          │\n",
    "│  YOLO Pose (2D)     │     │  Person ReID          │     │  3D Triangulation    │\n",
    "│  per-camera          │     │  cross-camera match   │     │  multi-camera fusion │\n",
    "└─────────────────────┘     └──────────────────────┘     └─────────────────────┘\n",
    "```\n",
    "\n",
    "**Why this order?** Each camera independently detects people, but has no idea which\n",
    "detection in Camera 1 is the same person as which detection in Camera 3. **ReID must\n",
    "come before triangulation** — otherwise triangulation doesn't know which 2D keypoints\n",
    "across cameras to match up into 3D points.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- A machine with a **CUDA-capable GPU** (recommended) or CPU\n",
    "- Python 3.8+ (3.10+ recommended)\n",
    "- Video files from your camera setup (`.mkv` or `.mp4`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup & Installation\n",
    "\n",
    "Before running any code, you need:\n",
    "\n",
    "1. **Conda** — a package/environment manager\n",
    "2. **A dedicated conda environment** with Python and CUDA support (for GPU acceleration)\n",
    "3. **The Python packages** used in this tutorial\n",
    "4. **Register the kernel** so Jupyter can use the environment\n",
    "\n",
    "> **Why conda?** Conda manages both Python packages *and* system-level CUDA libraries.\n",
    "> This means you do **not** need a system-wide CUDA install — conda handles it inside\n",
    "> your environment. This is the most reliable way to get GPU support across platforms.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1a: Install Conda (if you don't already have it)\n",
    "\n",
    "If you already have `conda` or `mamba` on your system, skip to Step 1b.\n",
    "\n",
    "We recommend **Miniforge** (includes `mamba`, a faster drop-in replacement for `conda`):\n",
    "\n",
    "| Platform | Download Link | Notes |\n",
    "|----------|--------------|-------|\n",
    "| **Windows** | [Miniforge3-Windows-x86_64.exe](https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Windows-x86_64.exe) | Run the `.exe` installer. Check \"Add to PATH\" when prompted. |\n",
    "| **macOS (Intel)** | [Miniforge3-MacOSX-x86_64.sh](https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-x86_64.sh) | Run in Terminal (see below) |\n",
    "| **macOS (Apple Silicon)** | [Miniforge3-MacOSX-arm64.sh](https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh) | For M1/M2/M3/M4 Macs |\n",
    "| **Linux (x86_64)** | [Miniforge3-Linux-x86_64.sh](https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh) | Most common for servers/workstations |\n",
    "| **Linux (aarch64)** | [Miniforge3-Linux-aarch64.sh](https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-aarch64.sh) | For ARM64 machines (e.g., Jetson) |\n",
    "\n",
    "**macOS / Linux install commands** (after downloading):\n",
    "\n",
    "```bash\n",
    "# Make the installer executable and run it\n",
    "bash Miniforge3-$(uname)-$(uname -m).sh\n",
    "\n",
    "# Follow the prompts (accept license, choose install location)\n",
    "# Then restart your terminal, or run:\n",
    "source ~/.bashrc   # Linux\n",
    "source ~/.zshrc    # macOS\n",
    "```\n",
    "\n",
    "**Verify installation:**\n",
    "\n",
    "```bash\n",
    "conda --version\n",
    "# Should print something like: conda 24.x.x\n",
    "```\n",
    "\n",
    "> **Alternative installers:**\n",
    "> - [Anaconda](https://www.anaconda.com/download) — larger distribution with GUI, 250+ packages pre-installed\n",
    "> - [Miniconda](https://docs.anaconda.com/miniconda/) — minimal installer from Anaconda (similar to Miniforge but uses the `defaults` channel)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1b: Create a Conda Environment with CUDA Support\n",
    "\n",
    "Open a terminal (or Anaconda Prompt on Windows) and run the command for **your platform**:\n",
    "\n",
    "#### Linux (with NVIDIA GPU)\n",
    "\n",
    "```bash\n",
    "conda create -n yolo-pose python=3.11 pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia -y\n",
    "conda activate yolo-pose\n",
    "pip install ultralytics opencv-python sleap-io numpy tqdm jupyter ipykernel\n",
    "```\n",
    "\n",
    "#### Windows (with NVIDIA GPU)\n",
    "\n",
    "```bash\n",
    "conda create -n yolo-pose python=3.11 pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia -y\n",
    "conda activate yolo-pose\n",
    "pip install ultralytics opencv-python sleap-io numpy tqdm jupyter ipykernel\n",
    "```\n",
    "\n",
    "#### macOS (Apple Silicon — M1/M2/M3/M4)\n",
    "\n",
    "```bash\n",
    "conda create -n yolo-pose python=3.11 pytorch torchvision -c pytorch -y\n",
    "conda activate yolo-pose\n",
    "pip install ultralytics opencv-python sleap-io numpy tqdm jupyter ipykernel\n",
    "```\n",
    "\n",
    "> **Note:** Apple Silicon Macs use **MPS** (Metal Performance Shaders) for GPU acceleration\n",
    "> instead of CUDA. PyTorch supports MPS automatically — no extra flags needed.\n",
    "> Performance is good but not as fast as a dedicated NVIDIA GPU.\n",
    "\n",
    "#### macOS (Intel)\n",
    "\n",
    "```bash\n",
    "conda create -n yolo-pose python=3.11 pytorch torchvision -c pytorch -y\n",
    "conda activate yolo-pose\n",
    "pip install ultralytics opencv-python sleap-io numpy tqdm jupyter ipykernel\n",
    "```\n",
    "\n",
    "> **Note:** Intel Macs have no GPU acceleration for PyTorch. Inference will run on CPU only.\n",
    "\n",
    "#### CPU Only (any platform, no GPU)\n",
    "\n",
    "```bash\n",
    "conda create -n yolo-pose python=3.11 pytorch torchvision cpuonly -c pytorch -y\n",
    "conda activate yolo-pose\n",
    "pip install ultralytics opencv-python sleap-io numpy tqdm jupyter ipykernel\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1c: Register the Kernel & Launch Jupyter\n",
    "\n",
    "After creating and activating the environment, you must **register it as a Jupyter kernel**\n",
    "so that Jupyter (and VSCode, JupyterLab, etc.) can find and use it:\n",
    "\n",
    "```bash\n",
    "conda activate yolo-pose\n",
    "python -m ipykernel install --user --name yolo-pose --display-name \"Python (yolo-pose)\"\n",
    "```\n",
    "\n",
    "Then launch Jupyter:\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "# or\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "In the Jupyter interface, select the **\"Python (yolo-pose)\"** kernel from the kernel picker\n",
    "(top-right in JupyterLab, or Kernel → Change Kernel in classic Notebook).\n",
    "\n",
    "> **VSCode users:** After registering the kernel, open the Command Palette\n",
    "> (`Ctrl+Shift+P` / `Cmd+Shift+P`) → \"Notebook: Select Notebook Kernel\" →\n",
    "> choose **\"Python (yolo-pose)\"**.\n",
    "\n",
    "---\n",
    "\n",
    "### Package Summary\n",
    "\n",
    "| Package | Purpose |\n",
    "|---|---|\n",
    "| `pytorch` + `torchvision` | Deep learning framework (with CUDA/MPS support) |\n",
    "| `ultralytics` | YOLO model for pose estimation |\n",
    "| `opencv-python` | Video reading, image manipulation |\n",
    "| `sleap-io` | Saving pose data in SLEAP format |\n",
    "| `numpy` | Numerical operations |\n",
    "| `tqdm` | Progress bars |\n",
    "| `jupyter` | Run this notebook |\n",
    "| `ipykernel` | Register this environment as a Jupyter kernel |\n",
    "\n",
    "### GPU Support Notes\n",
    "\n",
    "| Platform | GPU Backend | How to Check |\n",
    "|---|---|---|\n",
    "| Linux / Windows (NVIDIA) | **CUDA** | `nvidia-smi` in terminal |\n",
    "| macOS (Apple Silicon) | **MPS** | Automatic with PyTorch ≥ 1.12 |\n",
    "| macOS (Intel) | None | CPU only |\n",
    "\n",
    "Ultralytics automatically selects the best available device (CUDA → MPS → CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1a: Install packages (run this if not already installed)\n",
    "# ============================================================\n",
    "# If you followed the conda setup in the markdown cell above,\n",
    "# everything should already be installed. This cell is a fallback\n",
    "# for users who skipped the terminal setup or are running on\n",
    "# Google Colab / other hosted notebooks.\n",
    "#\n",
    "# Uncomment the section that matches your platform:\n",
    "\n",
    "# ── Google Colab (GPU runtime recommended) ───────────────────\n",
    "# !pip install ultralytics opencv-python sleap-io numpy tqdm\n",
    "\n",
    "# ── Linux / Windows with NVIDIA GPU (CUDA 12.1) ─────────────\n",
    "# !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install ultralytics opencv-python sleap-io numpy tqdm ipykernel\n",
    "\n",
    "# ── Linux / Windows CPU only ────────────────────────────────\n",
    "# !pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "# !pip install ultralytics opencv-python sleap-io numpy tqdm ipykernel\n",
    "\n",
    "# ── macOS (Apple Silicon or Intel) ──────────────────────────\n",
    "# !pip install torch torchvision\n",
    "# !pip install ultralytics opencv-python sleap-io numpy tqdm ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1b: Register this conda environment as a Jupyter kernel\n",
    "# ============================================================\n",
    "# If you created the \"yolo-pose\" conda environment but Jupyter\n",
    "# doesn't show it as a kernel option, run this cell to register it.\n",
    "#\n",
    "# You only need to do this ONCE per environment.\n",
    "# After running, restart Jupyter and select \"Python (yolo-pose)\"\n",
    "# from the kernel picker.\n",
    "\n",
    "# !pip install ipykernel\n",
    "\n",
    "# Uncomment and run:\n",
    "# !python -m ipykernel install --user --name yolo-pose --display-name \"Python (yolo-pose)\"\n",
    "\n",
    "# To verify which kernels are available:\n",
    "# !jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:   3.11.14 | packaged by conda-forge | (main, Jan 26 2026, 23:48:32) [GCC 14.3.0]\n",
      "Platform:         Linux x86_64\n",
      "\n",
      "PyTorch version:  2.5.1\n",
      "CUDA available:   True\n",
      "CUDA device:      NVIDIA A40\n",
      "CUDA version:     12.1\n",
      "Selected device:  cuda\n",
      "\n",
      "Ultralytics:      8.4.16\n",
      "OpenCV:           4.13.0\n",
      "SLEAP-IO:         0.6.4\n",
      "NumPy:            2.4.2\n",
      "\n",
      "All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 2: Verify installation and GPU availability\n",
    "# ============================================================\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "print(f\"Python version:   {sys.version}\")\n",
    "print(f\"Platform:         {platform.system()} {platform.machine()}\")\n",
    "print()\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version:  {torch.__version__}\")\n",
    "print(f\"CUDA available:   {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device:      {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version:     {torch.version.cuda}\")\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    print(f\"MPS available:    True (Apple Silicon GPU)\")\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Inference will run on CPU (slower).\")\n",
    "    device = \"cpu\"\n",
    "print(f\"Selected device:  {device}\")\n",
    "\n",
    "print()\n",
    "import ultralytics\n",
    "print(f\"Ultralytics:      {ultralytics.__version__}\")\n",
    "\n",
    "import cv2\n",
    "print(f\"OpenCV:           {cv2.__version__}\")\n",
    "\n",
    "import sleap_io as sio\n",
    "print(f\"SLEAP-IO:         {sio.__version__}\")\n",
    "\n",
    "import numpy as np\n",
    "print(f\"NumPy:            {np.__version__}\")\n",
    "\n",
    "print(\"\\nAll packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding the YOLO Pose Model\n",
    "\n",
    "### What is YOLOv8n-pose?\n",
    "\n",
    "YOLO (You Only Look Once) is a real-time object detection model. The **pose** variant adds\n",
    "**human skeleton keypoint estimation** on top of person detection.\n",
    "\n",
    "- **yolov8n-pose** — the \"nano\" (smallest/fastest) pose model\n",
    "- Detects people AND their 17 body keypoints in a single forward pass\n",
    "- Runs at ~100+ FPS on modern GPUs\n",
    "\n",
    "### The COCO 17-Keypoint Skeleton\n",
    "\n",
    "YOLO pose models use the **COCO keypoint format** with 17 joints:\n",
    "\n",
    "```\n",
    "Index  Name             Index  Name\n",
    "─────  ──────────────   ─────  ──────────────\n",
    "  0    nose               9    left_wrist\n",
    "  1    left_eye          10    right_wrist\n",
    "  2    right_eye         11    left_hip\n",
    "  3    left_ear          12    right_hip\n",
    "  4    right_ear         13    left_knee\n",
    "  5    left_shoulder     14    right_knee\n",
    "  6    right_shoulder    15    left_ankle\n",
    "  7    left_elbow        16    right_ankle\n",
    "  8    right_elbow\n",
    "```\n",
    "\n",
    "### Skeleton Connections (Edges)\n",
    "\n",
    "```\n",
    "         nose(0)\n",
    "        /     \\\n",
    "   l_eye(1)  r_eye(2)\n",
    "      |         |\n",
    "   l_ear(3)  r_ear(4)\n",
    "\n",
    "   l_shoulder(5)───r_shoulder(6)\n",
    "      |                  |\n",
    "   l_elbow(7)        r_elbow(8)\n",
    "      |                  |\n",
    "   l_wrist(9)        r_wrist(10)\n",
    "      |                  |\n",
    "    l_hip(11)──────r_hip(12)\n",
    "      |                  |\n",
    "   l_knee(13)        r_knee(14)\n",
    "      |                  |\n",
    "   l_ankle(15)       r_ankle(16)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: yolov8n-pose.pt\n",
      "Task: pose\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 3: Load the YOLO pose model\n",
    "# ============================================================\n",
    "# The first time you run this, it will download the model weights\n",
    "# (~6 MB for the nano model). Subsequent runs use the cached file.\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n-pose.pt\")  # nano pose model\n",
    "print(f\"Model loaded: {model.model_name}\")\n",
    "print(f\"Task: {model.task}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Extracting a Frame from Video\n",
    "\n",
    "Before running inference on entire videos, let's start by extracting a single frame.\n",
    "This is useful for:\n",
    "- Verifying the video loads correctly\n",
    "- Testing the model on one image\n",
    "- Quick visual inspection\n",
    "\n",
    "### How Timestamp-to-Frame Conversion Works\n",
    "\n",
    "```\n",
    "frame_number = timestamp_in_seconds × FPS\n",
    "```\n",
    "\n",
    "For example, to get the frame at 1h 28m 10s in a 30 FPS video:\n",
    "```\n",
    "seconds = 3600 + (28×60) + 10 = 5290\n",
    "frame   = 5290 × 30 = 158,700\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4: Extract a single frame from a video file\n",
    "# ============================================================\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# ── CONFIGURE THESE ──────────────────────────────────────────\n",
    "video_path = \"PUT_YOUR_VIDEO_HERE.mp4\"  # e.g. \"/path/to/CAM1_recording.mkv\"\n",
    "\n",
    "# Target timestamp: 1 hour, 28 minutes, 0 seconds\n",
    "hours, minutes, seconds = 1, 28, 0\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# Convert timestamp to total seconds\n",
    "timestamp_seconds = (hours * 3600) + (minutes * 60) + seconds\n",
    "print(f\"Target time: {hours}h {minutes}m {seconds}s = {timestamp_seconds} seconds\")\n",
    "\n",
    "# Open the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise FileNotFoundError(f\"Could not open video: {video_path}\")\n",
    "\n",
    "# Read video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "duration = total_frames / fps\n",
    "\n",
    "print(f\"\\nVideo properties:\")\n",
    "print(f\"  FPS:        {fps}\")\n",
    "print(f\"  Resolution: {width} x {height}\")\n",
    "print(f\"  Frames:     {total_frames:,}\")\n",
    "print(f\"  Duration:   {duration:.1f}s ({duration/60:.1f} min)\")\n",
    "\n",
    "# Calculate target frame number\n",
    "frame_number = int(timestamp_seconds * fps)\n",
    "print(f\"\\nTarget frame: {frame_number:,}\")\n",
    "\n",
    "# Seek to the target frame\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "# Read the frame\n",
    "success, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "if not success:\n",
    "    raise RuntimeError(f\"Could not read frame at {timestamp_seconds}s\")\n",
    "\n",
    "# Save the extracted frame\n",
    "frame_path = \"extracted_frame.jpg\"\n",
    "cv2.imwrite(frame_path, frame)\n",
    "print(f\"Frame saved to: {frame_path}\")\n",
    "print(f\"Frame shape: {frame.shape}  (height, width, channels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Single-Frame Pose Estimation\n",
    "\n",
    "Now let's run YOLO pose estimation on the extracted frame and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5: Run pose estimation on a single frame\n",
    "# ============================================================\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Run inference — returns a list of Results objects (one per image)\n",
    "results = model(\"extracted_frame.jpg\")\n",
    "\n",
    "# There's one result per image\n",
    "result = results[0]\n",
    "\n",
    "print(f\"Number of people detected: {len(result.boxes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6: Examine the detection results in detail\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "\n",
    "# --- Bounding Boxes ---\n",
    "# result.boxes contains detection bounding boxes\n",
    "boxes = result.boxes\n",
    "print(\"=\" * 60)\n",
    "print(\"BOUNDING BOXES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Format: [x1, y1, x2, y2] — top-left and bottom-right corners\")\n",
    "print(f\"Number of boxes: {len(boxes)}\")\n",
    "print(f\"\\nBox coordinates (xyxy):\")\n",
    "print(boxes.xyxy.cpu().numpy())\n",
    "print(f\"\\nConfidence scores:\")\n",
    "print(boxes.conf.cpu().numpy())\n",
    "print(f\"\\nClass IDs (0 = person):\")\n",
    "print(boxes.cls.cpu().numpy())\n",
    "\n",
    "# --- Keypoints ---\n",
    "# result.keypoints contains the 17 body keypoints per person\n",
    "kpts = result.keypoints\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEYPOINTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Keypoints shape: {kpts.xy.shape}\")\n",
    "print(f\"  → (num_people, 17_keypoints, 2_xy_coords)\")\n",
    "\n",
    "# Show keypoints for the first detected person\n",
    "if len(kpts.xy) > 0:\n",
    "    person_0_kpts = kpts.xy[0].cpu().numpy()\n",
    "    person_0_conf = kpts.conf[0].cpu().numpy() if kpts.conf is not None else None\n",
    "    \n",
    "    node_names = [\n",
    "        \"nose\", \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\",\n",
    "        \"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\",\n",
    "        \"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\",\n",
    "        \"left_knee\", \"right_knee\", \"left_ankle\", \"right_ankle\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nPerson 0 keypoints:\")\n",
    "    print(f\"{'Index':<6} {'Name':<18} {'X':>8} {'Y':>8} {'Conf':>8}\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, name in enumerate(node_names):\n",
    "        x, y = person_0_kpts[i]\n",
    "        conf = person_0_conf[i] if person_0_conf is not None else float('nan')\n",
    "        visible = \"✓\" if not (x == 0 and y == 0) else \"✗\"\n",
    "        print(f\"{i:<6} {name:<18} {x:>8.1f} {y:>8.1f} {conf:>8.3f} {visible}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 7: Visualize the detection inline in the notebook\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Get annotated frame with bounding boxes and skeleton overlay\n",
    "annotated = results[0].plot()  # returns BGR numpy array\n",
    "\n",
    "# Convert BGR → RGB for matplotlib\n",
    "annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
    "ax.imshow(annotated_rgb)\n",
    "ax.set_title(f\"YOLO Pose Detection — {len(results[0].boxes)} people detected\", fontsize=14)\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also save to file for reference\n",
    "results[0].save(filename=\"detected_frame.jpg\")\n",
    "print(\"Also saved to: detected_frame.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Processing an Entire Video\n",
    "\n",
    "Now we scale up from a single frame to processing every frame in a video.\n",
    "\n",
    "### Strategy\n",
    "\n",
    "1. Use `model.predict(video_path, stream=True)` for memory-efficient streaming\n",
    "2. For each frame, extract keypoints, boxes, and confidence scores\n",
    "3. Store results in both **SLEAP format** (.slp) and **JSON/Pickle** for flexibility\n",
    "\n",
    "### Output Formats\n",
    "\n",
    "| Format | File | Use Case |\n",
    "|---|---|---|\n",
    "| SLEAP (.slp) | `*_pose.slp` | Visual inspection in SLEAP GUI, downstream triangulation |\n",
    "| JSON | `*_results.json` | Human-readable, cross-language |\n",
    "| Pickle | `*_results.pkl` | Fast Python loading, preserves types |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 8: Define the SLEAP skeleton structure\n",
    "# ============================================================\n",
    "# We need to define the skeleton so SLEAP knows how to connect\n",
    "# the keypoints when we save the results.\n",
    "\n",
    "import sleap_io as sio\n",
    "import numpy as np\n",
    "\n",
    "# COCO 17-keypoint names\n",
    "node_names = [\n",
    "    \"nose\", \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\",\n",
    "    \"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\",\n",
    "    \"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\",\n",
    "    \"left_knee\", \"right_knee\", \"left_ankle\", \"right_ankle\"\n",
    "]\n",
    "\n",
    "# COCO skeleton edge connections (pairs of keypoint indices)\n",
    "edge_inds = [\n",
    "    [0, 1], [0, 2],   # nose → eyes\n",
    "    [1, 3], [2, 4],   # eyes → ears\n",
    "    [5, 7], [7, 9],   # left arm: shoulder → elbow → wrist\n",
    "    [6, 8], [8, 10],  # right arm: shoulder → elbow → wrist\n",
    "    [5, 6],           # shoulder to shoulder\n",
    "    [5, 11], [6, 12], # shoulders → hips\n",
    "    [11, 12],         # hip to hip\n",
    "    [11, 13], [13, 15],  # left leg: hip → knee → ankle\n",
    "    [12, 14], [14, 16]   # right leg: hip → knee → ankle\n",
    "]\n",
    "\n",
    "# Create the SLEAP skeleton\n",
    "skeleton = sio.Skeleton(nodes=node_names, edges=edge_inds)\n",
    "print(f\"Skeleton created with {len(node_names)} nodes and {len(edge_inds)} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 9: Process an entire video — standard (landscape) camera\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import sleap_io as sio\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# ── CONFIGURE THESE ──────────────────────────────────────────\n",
    "video_path = \"PUT_YOUR_LANDSCAPE_VIDEO_HERE.mp4\"  # e.g. \"/path/to/CAM1_video.mp4\"\n",
    "output_dir = \"pose_results\"\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "video_name = Path(video_path).stem\n",
    "\n",
    "# Load model\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Create SLEAP objects\n",
    "video_obj = sio.Video(filename=video_path)\n",
    "labels = sio.Labels(videos=[video_obj], skeletons=[skeleton])\n",
    "\n",
    "# Storage for raw Ultralytics results\n",
    "ultralytics_results = []\n",
    "\n",
    "# Get video properties\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "vid_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "vid_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "cap.release()\n",
    "\n",
    "print(f\"Processing: {video_name}\")\n",
    "print(f\"Total frames: {total_frames:,}\")\n",
    "print(f\"Video resolution: {vid_w}x{vid_h}\")\n",
    "\n",
    "# Stream inference (memory-efficient: processes one frame at a time)\n",
    "results = model.predict(video_path, stream=True, verbose=False)\n",
    "\n",
    "for frame_idx, result in tqdm(enumerate(results), total=total_frames, desc=\"Processing\"):\n",
    "    \n",
    "    # ── Store raw results ──\n",
    "    frame_result = {\n",
    "        'frame_idx': frame_idx,\n",
    "        'keypoints': result.keypoints.xy.cpu().numpy().tolist()\n",
    "            if hasattr(result, 'keypoints') and result.keypoints is not None else [],\n",
    "        'keypoints_conf': result.keypoints.conf.cpu().numpy().tolist()\n",
    "            if hasattr(result, 'keypoints') and result.keypoints is not None\n",
    "            and hasattr(result.keypoints, 'conf') else [],\n",
    "        'boxes': result.boxes.xyxy.cpu().numpy().tolist()\n",
    "            if hasattr(result, 'boxes') and result.boxes is not None else [],\n",
    "        'boxes_conf': result.boxes.conf.cpu().numpy().tolist()\n",
    "            if hasattr(result, 'boxes') and result.boxes is not None else [],\n",
    "        'boxes_cls': result.boxes.cls.cpu().numpy().tolist()\n",
    "            if hasattr(result, 'boxes') and result.boxes is not None else []\n",
    "    }\n",
    "    ultralytics_results.append(frame_result)\n",
    "    \n",
    "    # ── Build SLEAP labeled frame ──\n",
    "    labeled_frame = sio.LabeledFrame(video=video_obj, frame_idx=frame_idx)\n",
    "    \n",
    "    detections = result.keypoints.xy.cpu().numpy() \\\n",
    "        if hasattr(result, 'keypoints') and result.keypoints is not None else []\n",
    "    \n",
    "    if len(detections) == 0:\n",
    "        continue\n",
    "    \n",
    "    for person_idx, keypoints in enumerate(detections):\n",
    "        # Build points array: NaN for invisible keypoints, (x,y) for visible\n",
    "        points_arr = np.zeros((len(keypoints), 2))\n",
    "        for kp_idx, kp in enumerate(keypoints):\n",
    "            x, y = float(kp[0]), float(kp[1])\n",
    "            if x == 0 and y == 0:\n",
    "                points_arr[kp_idx] = [np.nan, np.nan]  # not visible\n",
    "            else:\n",
    "                points_arr[kp_idx] = [x, y]\n",
    "        \n",
    "        instance = sio.Instance.from_numpy(points_arr, skeleton)\n",
    "        labeled_frame.instances.append(instance)\n",
    "    \n",
    "    labels.append(labeled_frame)\n",
    "\n",
    "# ── Save all output formats ──\n",
    "slp_path = f\"{output_dir}/{video_name}_pose.slp\"\n",
    "json_path = f\"{output_dir}/{video_name}_ultralytics_results.json\"\n",
    "pkl_path = f\"{output_dir}/{video_name}_ultralytics_results.pkl\"\n",
    "\n",
    "labels.save(slp_path)\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(ultralytics_results, f, indent=2)\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(ultralytics_results, f)\n",
    "\n",
    "print(f\"\\nDone! Outputs saved:\")\n",
    "print(f\"  SLEAP:  {slp_path}\")\n",
    "print(f\"  JSON:   {json_path}\")\n",
    "print(f\"  Pickle: {pkl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 9b: Visualize sample frames from the processed video\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick a few evenly-spaced frames to visualize\n",
    "n_samples = 4\n",
    "sample_indices = np.linspace(0, len(ultralytics_results) - 1, n_samples, dtype=int)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "model_viz = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "fig, axes = plt.subplots(1, n_samples, figsize=(20, 5))\n",
    "\n",
    "for i, fidx in enumerate(sample_indices):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, fidx)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    \n",
    "    # Run inference on this frame (single frame, no stream)\n",
    "    res = model_viz.predict(frame, verbose=False)[0]\n",
    "    annotated = cv2.cvtColor(res.plot(), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    axes[i].imshow(annotated)\n",
    "    n_people = len(res.boxes)\n",
    "    axes[i].set_title(f\"Frame {fidx} — {n_people} people\", fontsize=10)\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "cap.release()\n",
    "\n",
    "fig.suptitle(f\"Sample detections from: {video_name}\", fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Handling Portrait (Rotated) Cameras\n",
    "\n",
    "Some cameras may be mounted in **portrait orientation** (e.g., CAM4 in our setup).\n",
    "YOLO performs best on landscape-oriented images, so we need to:\n",
    "\n",
    "1. **Rotate the frame 90° clockwise** before inference (portrait → landscape)\n",
    "2. **Pad the height** to a stride-32 multiple (YOLO requirement)\n",
    "3. **Run inference** on the rotated frame\n",
    "4. **Map keypoints back** to original portrait coordinates\n",
    "\n",
    "### Coordinate Mapping\n",
    "\n",
    "When rotating 90° CW:\n",
    "```\n",
    "Portrait (x_p, y_p)  ──rotate 90° CW──►  Landscape (x_r, y_r)\n",
    "  x_r = y_p\n",
    "  y_r = W_portrait - x_p\n",
    "\n",
    "To reverse (landscape → portrait):\n",
    "  x_p = y_r\n",
    "  y_p = W_rotated - x_r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 10: Process a portrait/rotated camera video\n",
    "# ============================================================\n",
    "import math\n",
    "\n",
    "def rotate_back_cw90(kpts_xy, rot_shape):\n",
    "    \"\"\"\n",
    "    Convert keypoints detected on a 90°-CW-rotated frame\n",
    "    back to the original portrait coordinate system.\n",
    "    \n",
    "    Parameters:\n",
    "        kpts_xy:   array of shape (N, 2) — keypoint (x, y) in rotated frame\n",
    "        rot_shape: tuple (h, w) of the rotated frame (before padding)\n",
    "    \n",
    "    Returns:\n",
    "        array of shape (N, 2) — keypoints in original portrait coordinates\n",
    "    \"\"\"\n",
    "    h, w = rot_shape\n",
    "    x_r, y_r = kpts_xy[:, 0], kpts_xy[:, 1]\n",
    "    x_p = y_r           # portrait x = rotated y\n",
    "    y_p = w - x_r       # portrait y = rotated width - rotated x\n",
    "    return np.stack([x_p, y_p], axis=-1)\n",
    "\n",
    "\n",
    "# ── CONFIGURE ──\n",
    "video_path = \"PUT_YOUR_PORTRAIT_VIDEO_HERE.mp4\"  # e.g. \"/path/to/CAM4_portrait_video.mp4\"\n",
    "output_dir = \"pose_results\"\n",
    "# ───────────────\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "video_name = Path(video_path).stem\n",
    "\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "video_obj = sio.Video(filename=video_path)\n",
    "labels = sio.Labels(videos=[video_obj], skeletons=[skeleton])\n",
    "ultra_results = []\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"Processing portrait camera: {video_name}\")\n",
    "print(f\"Total frames: {total_frames:,}\")\n",
    "\n",
    "for idx in tqdm(range(total_frames), desc=\"Processing\"):\n",
    "    ret, frame_portrait = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # 1. Rotate 90° CW: portrait → landscape\n",
    "    frame_rotated = cv2.rotate(frame_portrait, cv2.ROTATE_90_CLOCKWISE)\n",
    "    rh, rw = frame_rotated.shape[:2]\n",
    "    \n",
    "    # 2. Run inference on rotated frame\n",
    "    result = model.predict(frame_rotated, verbose=False)[0]\n",
    "    \n",
    "    # Store raw results\n",
    "    ultra_results.append({\n",
    "        \"frame_idx\": idx,\n",
    "        \"keypoints\": result.keypoints.xy.cpu().numpy().tolist()\n",
    "            if hasattr(result, 'keypoints') and result.keypoints is not None else [],\n",
    "        \"keypoints_conf\": result.keypoints.conf.cpu().numpy().tolist()\n",
    "            if hasattr(result, 'keypoints') and result.keypoints is not None\n",
    "            and hasattr(result.keypoints, 'conf') else [],\n",
    "        \"boxes\": result.boxes.xyxy.cpu().numpy().tolist()\n",
    "            if hasattr(result, 'boxes') and result.boxes is not None else [],\n",
    "        \"boxes_conf\": result.boxes.conf.cpu().numpy().tolist()\n",
    "            if hasattr(result, 'boxes') and result.boxes is not None else [],\n",
    "        \"boxes_cls\": result.boxes.cls.cpu().numpy().tolist()\n",
    "            if hasattr(result, 'boxes') and result.boxes is not None else []\n",
    "    })\n",
    "    \n",
    "    # 3. Map keypoints back to portrait coordinates\n",
    "    dets = (result.keypoints.xy.cpu().numpy()\n",
    "            if hasattr(result, 'keypoints') and result.keypoints is not None else [])\n",
    "    \n",
    "    if len(dets):\n",
    "        lf = sio.LabeledFrame(video=video_obj, frame_idx=idx)\n",
    "        for kpts in dets:\n",
    "            kpts_portrait = rotate_back_cw90(kpts, (rh, rw))\n",
    "            pts = np.where(\n",
    "                (kpts_portrait == 0).all(axis=1, keepdims=True),\n",
    "                np.nan, kpts_portrait\n",
    "            )\n",
    "            lf.instances.append(sio.Instance.from_numpy(pts, skeleton))\n",
    "        labels.append(lf)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Save outputs\n",
    "slp_path = f\"{output_dir}/{video_name}_pose.slp\"\n",
    "json_path = f\"{output_dir}/{video_name}_ultralytics_results.json\"\n",
    "pkl_path = f\"{output_dir}/{video_name}_ultralytics_results.pkl\"\n",
    "\n",
    "labels.save(slp_path)\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(ultra_results, f, indent=2)\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(ultra_results, f)\n",
    "\n",
    "print(f\"\\nDone! Portrait camera outputs saved:\")\n",
    "print(f\"  SLEAP:  {slp_path}\")\n",
    "print(f\"  JSON:   {json_path}\")\n",
    "print(f\"  Pickle: {pkl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultra_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 11: Batch process multiple camera videos\n",
    "# ============================================================\n",
    "\n",
    "# Define all your video paths (landscape cameras only — process portrait cameras with Step 10)\n",
    "video_paths = [\n",
    "    \"PUT_CAM_A_VIDEO_HERE.mp4\",\n",
    "    \"PUT_CAM_B_VIDEO_HERE.mp4\",\n",
    "    \"PUT_CAM_C_VIDEO_HERE.mp4\",\n",
    "    # \"PUT_CAM_D_VIDEO_HERE.mp4\",  # portrait — use Step 10 instead\n",
    "    \"PUT_CAM_E_VIDEO_HERE.mp4\",\n",
    "    \"PUT_CAM_F_VIDEO_HERE.mp4\",\n",
    "]\n",
    "\n",
    "output_dir = \"pose_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "for video_path in video_paths:\n",
    "    video_name = Path(video_path).stem\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {video_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    video_obj = sio.Video(filename=video_path)\n",
    "    labels = sio.Labels(videos=[video_obj], skeletons=[skeleton])\n",
    "    ultralytics_results = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    vid_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    vid_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "    \n",
    "    print(f\"  Resolution: {vid_w}x{vid_h}\")\n",
    "    \n",
    "    results = model.predict(video_path, stream=True, verbose=False)\n",
    "    \n",
    "    for frame_idx, result in tqdm(enumerate(results), total=total_frames):\n",
    "        frame_result = {\n",
    "            'frame_idx': frame_idx,\n",
    "            'keypoints': result.keypoints.xy.cpu().numpy().tolist()\n",
    "                if hasattr(result, 'keypoints') and result.keypoints is not None else [],\n",
    "            'keypoints_conf': result.keypoints.conf.cpu().numpy().tolist()\n",
    "                if hasattr(result, 'keypoints') and result.keypoints is not None\n",
    "                and hasattr(result.keypoints, 'conf') else [],\n",
    "            'boxes': result.boxes.xyxy.cpu().numpy().tolist()\n",
    "                if hasattr(result, 'boxes') and result.boxes is not None else [],\n",
    "            'boxes_conf': result.boxes.conf.cpu().numpy().tolist()\n",
    "                if hasattr(result, 'boxes') and result.boxes is not None else [],\n",
    "            'boxes_cls': result.boxes.cls.cpu().numpy().tolist()\n",
    "                if hasattr(result, 'boxes') and result.boxes is not None else []\n",
    "        }\n",
    "        ultralytics_results.append(frame_result)\n",
    "        \n",
    "        labeled_frame = sio.LabeledFrame(video=video_obj, frame_idx=frame_idx)\n",
    "        detections = result.keypoints.xy.cpu().numpy() \\\n",
    "            if hasattr(result, 'keypoints') and result.keypoints is not None else []\n",
    "        \n",
    "        if len(detections) > 0:\n",
    "            for keypoints in detections:\n",
    "                points_arr = np.zeros((len(keypoints), 2))\n",
    "                for kp_idx, kp in enumerate(keypoints):\n",
    "                    x, y = float(kp[0]), float(kp[1])\n",
    "                    points_arr[kp_idx] = [np.nan, np.nan] if (x == 0 and y == 0) else [x, y]\n",
    "                labeled_frame.instances.append(sio.Instance.from_numpy(points_arr, skeleton))\n",
    "            labels.append(labeled_frame)\n",
    "    \n",
    "    # Save\n",
    "    labels.save(f\"{output_dir}/{video_name}_pose.slp\")\n",
    "    with open(f\"{output_dir}/{video_name}_ultralytics_results.json\", 'w') as f:\n",
    "        json.dump(ultralytics_results, f, indent=2)\n",
    "    with open(f\"{output_dir}/{video_name}_ultralytics_results.pkl\", 'wb') as f:\n",
    "        pickle.dump(ultralytics_results, f)\n",
    "    \n",
    "    print(f\"  Saved {len(ultralytics_results)} frames\")\n",
    "\n",
    "print(\"\\nAll cameras processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 12: Visualize one synchronized frame across all cameras\n",
    "# ============================================================\n",
    "# Since the videos are time-aligned, frame 0 in each video\n",
    "# corresponds to the same moment in time. This grid shows what\n",
    "# all 6 cameras see simultaneously.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "model_viz = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# All camera videos (update these to match your video_paths from Steps 9-11)\n",
    "all_camera_paths = [\n",
    "    \"PUT_CAM_A_VIDEO_HERE.mp4\",\n",
    "    \"PUT_CAM_B_VIDEO_HERE.mp4\",\n",
    "    \"PUT_CAM_C_VIDEO_HERE.mp4\",\n",
    "    \"PUT_CAM_D_VIDEO_HERE.mp4\",\n",
    "    \"PUT_CAM_E_VIDEO_HERE.mp4\",\n",
    "    \"PUT_CAM_F_VIDEO_HERE.mp4\",\n",
    "]\n",
    "camera_labels = [\"CAM A\", \"CAM B\", \"CAM C\", \"CAM D (portrait)\", \"CAM E\", \"CAM F\"]\n",
    "portrait_indices = {3}  # set of indices for portrait-oriented cameras\n",
    "\n",
    "target_frame = 0  # all videos are aligned, so frame 0 = same moment\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(24, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (vpath, label) in enumerate(zip(all_camera_paths, camera_labels)):\n",
    "    cap = cv2.VideoCapture(vpath)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if not ret:\n",
    "        axes[i].set_title(f\"{label} — FAILED TO READ\", fontsize=12)\n",
    "        axes[i].axis(\"off\")\n",
    "        continue\n",
    "    \n",
    "    # For portrait cameras, rotate before inference\n",
    "    if i in portrait_indices:\n",
    "        frame_for_model = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "    else:\n",
    "        frame_for_model = frame\n",
    "    \n",
    "    res = model_viz.predict(frame_for_model, verbose=False)[0]\n",
    "    annotated = cv2.cvtColor(res.plot(), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # For portrait cameras, rotate the annotated image back for display\n",
    "    if i in portrait_indices:\n",
    "        annotated = cv2.rotate(annotated, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    \n",
    "    axes[i].imshow(annotated)\n",
    "    n_people = len(res.boxes)\n",
    "    axes[i].set_title(f\"{label} — {n_people} people\", fontsize=12)\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"Synchronized Frame 0 — All Cameras with Pose Detection\", fontsize=15, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo-pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
